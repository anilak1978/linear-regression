---
title: "Chapter 8 - Introduction to Linear Regression"
author: ''
output:
  html_document:
    df_print: paged
  pdf_document:
    extra_dependencies:
    - geometry
    - multicol
    - multirow
    - xcolor
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Nutrition at Starbucks, Part I.** (8.22, p. 326) The scatterplot below shows the relationship between the number of calories and amount of carbohydrates (in grams) Starbucks food menu items contain. Since Starbucks only lists the number of calories on the display items, we are interested in predicting the amount of carbs a menu item has based on its calorie content.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.show="hold", out.width="33%", fig.height=4}
library(openintro)
# load data ---------------------------------------------------------
starbucks <- read.csv("https://github.com/jbryer/DATA606Fall2019/raw/master/course_data/starbucks.csv")
# model calories vs. carbos -----------------------------------------
m_carb_cals <- lm(carb ~ calories, data = starbucks)
# plot calories vs. carbos ------------------------------------------
par(mar = c(3.5, 4, 1, 0.5), las = 1, mgp = c(2.5, 0.7, 0), 
    cex.lab = 1.5, cex.axis = 1.5)
plot(carb ~ calories, data = starbucks, 
     pch = 19, col = COL[1,2], 
     xlab = "Calories", ylab = "Carbs (grams)", axes = FALSE)
axis(1)
axis(2, at = seq(20, 80, 20))
box()
abline(m_carb_cals, col = COL[2], lwd = 2)
# plot residuals ----------------------------------------------------
par(mar = c(3.5, 4, 1, 0.5), las = 1, mgp = c(2.5, 0.7, 0), 
    cex.lab = 1.5, cex.axis = 1.5)

plot(m_carb_cals$residuals ~ starbucks$calories,
     xlab = "Calories", ylab = "Residuals", 
     col = COL[1,2], pch = 19,
     ylim = c(-30, 30), axes = FALSE)
axis(1)
axis(2, at = seq(-20, 20, 20))
box()
abline(h = 0, lty = 2)
# histogram of residuals --------------------------------------------
par(mar = c(3.5, 2.5, 0.5, 0.5), las = 1, mgp = c(2.5, 0.7, 0), 
    cex.lab = 1.5, cex.axis = 1.5)

hist(m_carb_cals$residuals,
     col = COL[1], 
     xlab = "Residuals", ylab = "", main = "", 
     axes = FALSE, xlim = c(-40,40))
axis(1, at = seq(-40, 40, 20))
axis(2)
```

(a) Describe the relationship between number of calories and amount of carbohydrates (in grams) that Starbucks food menu items contain.
(b) In this scenario, what are the explanatory and response variables?
(c) Why might we want to fit a regression line to these data?
(d) Do these data meet the conditions required for fitting a least squares line?

# Answer Nutrition at Starbucks, Part 1.

(a):

There is a linear positive relationship between Calories and Carbs. The relationship is not strong nor weak. (I would say something in the middle.)

(b):

The response variable is Carbs(grams), the explanatory variable is Calories.

(c):

If our goal requires to predict the response variable by using the explanatory variable , see residuals and linear relationship between the two variables, we may want to fit a regression line to these data. 

(d):

Conditions for the least squares lines are:

1- Linearity : The releationship between the explanatory and the response variable should be linear.  

2- Nearly Normal Residuals: The residuals should be nearly normal. This condition may not be satisfied when there are unusual observations that dont follow the trend of the rest of the data. We can use histogram (as per above) to check the normality. 

3- Constant Variability: The variability of points around the least squares line should be roughly constant. This implies that the variability of residuals around the 0 line should be roughly constant as well. We can check constant variability by checking residual plot. (as above)




--------------------------------------------------------------------------------

\clearpage

**Body measurements, Part I.** (8.13, p. 316) Researchers studying anthropometry collected body girth measurements and skeletal diameter measurements, as well as age, weight, height and gender for 507 physically active individuals.19 The scatterplot below shows the relationship between height and shoulder girth (over deltoid muscles), both measured in centimeters.

\begin{center}
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.show="hold", out.width="50%", fig.height=4}
# load packages -----------------------------------------------------
library(openintro)
# load data ---------------------------------------------------------
data(bdims)
# plot height vs. shoulder girth ------------------------------------
par(mar = c(3.8, 3.8, 0.5, 0.5), las = 1, mgp = c(2.7, 0.7, 0), 
    cex.lab = 1.25, cex.axis = 1.25)
plot(bdims$hgt ~ bdims$sho.gi, 
     xlab = "Shoulder girth (cm)", ylab = "Height (cm)", 
     pch = 19, col = COL[1,2])
```
\end{center}

(a) Describe the relationship between shoulder girth and height.
(b) How would the relationship change if shoulder girth was measured in inches while the units of height remained in centimeters?


# Answer Body Measurements, Part 1.

(a):

The response variable is height(cm), the explanatory variable is shoulder girth(cm). There is a semi strong, positive, linear relationship between these two variables. 

(b):

```{r}

bdims$sho.gi <- bdims$sho.gi*0.393701 # convert cm to inches
plot(bdims$hgt ~ bdims$sho.gi, 
     xlab = "Shoulder girth (inch)", ylab = "Height (cm)", 
     pch = 19, col = COL[1,2])

```

The relationship stays the same. Semi strong, postive linear relationsip between shoulder girth and height.


--------------------------------------------------------------------------------

\clearpage
                                      
**Body measurements, Part III.** (8.24, p. 326) Exercise above introduces data on shoulder girth and height of a group of individuals. The mean shoulder girth is 107.20 cm with a standard deviation of 10.37 cm. The mean height is 171.14 cm with a standard deviation of 9.41 cm. The correlation between height and shoulder girth is 0.67.

(a) Write the equation of the regression line for predicting height.
(b) Interpret the slope and the intercept in this context.
(c) Calculate $R^2$ of the regression line for predicting height from shoulder girth, and interpret it in the context of the application.
(d) A randomly selected student from your class has a shoulder girth of 100 cm. Predict the height of this student using the model.
(e) The student from part (d) is 160 cm tall. Calculate the residual, and explain what this residual means.
(f) A one year old has a shoulder girth of 56 cm. Would it be appropriate to use this linear model to predict the height of this child?



# Answer Body Measurements, Part 3.

Slope Calculation:

$b_{1}= (s_{y}/s_{x})R$


Intercept: 

$b_{0}=\overline{y}-b_{1}\overline{x}$


```{r}
# calculating slope and intercept
# shoulder girth is explanatory and height is response variable

mean_shoulder <- 107.20
mean_height <- 171.14
sd_shoulder <- 10.37
sd_height <- 9.41
r <- 0.67

slope <- (sd_height/sd_shoulder)*r
intercept <- mean_height- (slope*mean_shoulder)

slope
intercept

```

predicted height = 105.96651 + (0.6079749 * shoulder girth)

(b):

slope = 0.60

For each additional shoulder girth value , we would expect the cm in height increases on average by 0.60 cms.

intercept: 105.96

The intercept is where the regression line intersects the y-axis. The individuals with no shoulder girth (unfortunately does not make sense but is the interpretation of the intercept), are expected to have 105.96 average height in cms. 

(c):

$R^2$ --> Strength of the fit of a linear model, calculated as the square of the correlation coefficient.

```{r}

rsquared <- 0.67^2
rsquared

```

44% of the variability in height , explained by shoulder girth. 

(d):

```{r}

predicted_height <- intercept + (slope * 100)
predicted_height

```

(d):

```{r}
residual <- 160-predicted_height
residual

```

Our model predicted the height wrong by 6.76. Overestimated the height of the person. 

(e):

56 cm of shoulder girth is not part of the dataset, so it wouldnt be appropriate to use this in the linear model.


--------------------------------------------------------------------------------

\clearpage

**Cats, Part I.** (8.26, p. 327) The following regression output is for predicting the heart weight (in g) of cats from their body weight (in kg). The coefficients are estimated using a dataset of 144 domestic cats.

\begin{center}
{
\begin{tabular}{rrrrr}
    \hline
            & Estimate  & Std. Error    & t value   & Pr($>$$|$t$|$) \\ 
    \hline
(Intercept) & -0.357    & 0.692         & -0.515    & 0.607 \\ 
body wt     & 4.034     & 0.250         & 16.119    & 0.000 \\ 
    \hline
\end{tabular} \ \\
$s = 1.452 \qquad R^2 = 64.66\% \qquad R^2_{adj} = 64.41\%$ 
}
\end{center}

\begin{center}
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.show="hold", out.width="50%", fig.height=4}
# load packages -----------------------------------------------------
library(openintro)
library(xtable)
library(MASS)
# load data ---------------------------------------------------------
data(cats)
# model heart weight vs. weight -------------------------------------
m_cats_hwt_bwt <- lm(cats$Hwt ~ cats$Bwt)
# plot heart weight vs. weight --------------------------------------
par(mar = c(3.7, 3.7, 0.5, 0.5), las = 1, mgp = c(2.5, 0.7, 0), 
    cex.lab = 1.5, cex.axis = 1.5)
plot(cats$Hwt ~ cats$Bwt, 
     xlab = "Body weight (kg)", ylab = "Heart weight (g)", 
     pch = 19, col = COL[1,2],
     xlim = c(2,4), ylim = c(5, 20.5), axes = FALSE)
axis(1, at = seq(2, 4, 0.5))
axis(2, at = seq(5, 20, 5))
box()
```
\end{center}

(a) Write out the linear model.
(b) Interpret the intercept.
(c) Interpret the slope.
(d) Interpret $R^2$.
(e) Calculate the correlation coefficient.


# Answer Cats Part 1.

(a):

predicted heart weight = intercept + (slope*Body_weight(kg))

predicted heart weight =  -0.357 + (4.034*Body_weight)

(b):

The individuals with no body weight (unfortunately does not make sense but is the interpretation of the intercept), are expected to have -0.357 average heart weight.

(c):

For each additional body weight kg , we would expect the gram in heart weight to increases on average by 4.034.

(d):

64.66% of the variability in heart weight , explained by body weight.

(e):

```{r}

corr <- sqrt(0.6466)
corr

```

--------------------------------------------------------------------------------

\clearpage

**Rate my professor.** (8.44, p. 340) Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, such as the physical appearance of the instructor. Researchers at University of Texas, Austin collected data on teaching evaluation score (higher score means better) and standardized beauty score (a score of 0 means average, negative score means below average, and a positive score means above average) for a sample of 463 professors. The scatterplot below shows the relationship between these variables, and also provided is a regression output for predicting teaching evaluation score from beauty score.

\begin{center}
\begin{tabular}{rrrrr}
    \hline
            & Estimate  & Std. Error    & t value   & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 4.010     & 0.0255        & 	157.21  & 0.0000 \\ 
beauty      &  \fbox{\textcolor{white}{{ Cell 1}}}  
                        & 0.0322        & 4.13      & 0.0000\vspace{0.8mm} \\ 
   \hline
\end{tabular}


```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.show="hold", out.width="50%", fig.height=4}
# load packages -----------------------------------------------------
library(openintro)
# load data ---------------------------------------------------------
prof_evals_beauty <- read.csv("https://github.com/jbryer/DATA606Fall2019/raw/master/course_data/prof_evals_beauty.csv")
# rename variables for convenience ----------------------------------
beauty <- prof_evals_beauty$btystdave
eval <- prof_evals_beauty$courseevaluation
# model evaluation scores vs. beauty --------------------------------
m_eval_beauty = lm(eval ~ beauty)
# scatterplot of evaluation scores vs. beauty -----------------------
par(mar = c(3.9, 3.9, 0.5, 0.5), las = 0, mgp = c(2.7, 0.7, 0), 
    cex.lab = 1.5, cex.axis = 1.5, las = 1)
plot(eval ~ beauty, 
     xlab = "Beauty", ylab = "Teaching evaluation", 
     pch = 19, col = COL[1,2], 
     axes = FALSE)
axis(1, at = seq(-1, 2, 1))
axis(2, at = seq(2, 5, 1))
box()
```
\end{center}

(a) Given that the average standardized beauty score is -0.0883 and average teaching evaluation score is 3.9983, calculate the slope. Alternatively, the slope may be computed using just the information provided in the model summary table.
(b) Do these data provide convincing evidence that the slope of the relationship between teaching evaluation and beauty is positive? Explain your reasoning.
(c) List the conditions required for linear regression and check if each one is satisfied for this model based on the following diagnostic plots.




```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.show="hold", out.width="50%", fig.height=4}
# residuals plot ----------------------------------------------------
par(mar = c(3.9, 3.9, 0.5, 0.5), las = 0, mgp = c(2.7, 0.7, 0), 
    cex.lab = 1.5, cex.axis = 1.5, las = 1)
plot(m_eval_beauty$residuals ~ beauty, 
     xlab = "Beauty", ylab = "Residuals", 
     pch = 19, col = COL[1,2], 
     ylim = c(-1.82, 1.82), axes = FALSE)
axis(1, at = seq(-1, 2, 1))
axis(2, at = seq(-1, 1, 1))
box()
abline(h = 0, lty = 3)
# residuals histogram -----------------------------------------------
par(mar = c(3.9, 3, 0, 0), cex.lab = 1.5, cex.axis = 1.5)
hist(m_eval_beauty$residuals, 
     xlab = "Residuals", ylab = "", main = "",
     col = COL[1],
     xlim = c(-2,2))
# normal probability plot of residuals ------------------------------
par(mar = c(3.9, 3.9, 0.5, 0.5), mgp = c(2.7, 0.7, 0), 
    cex.lab = 1.5, cex.axis = 1.5)
qqnorm(m_eval_beauty$residuals, 
       pch = 19, col = COL[1,2],
       main = "", las = 0)
qqline(m_eval_beauty$residuals, col = COL[1])
# order of residuals ---------------------------------------------===
par(mar = c(3.9, 3.9, 0.5, 0.5), mgp = c(2.7, 0.7, 0), 
    cex.lab = 1.5, cex.axis = 1.5)
plot(m_eval_beauty$residuals, 
     xlab = "Order of data collection", ylab = "Residuals", main = "",
     pch = 19, col = COL[1,2],
     ylim = c(-1.82, 1.82), axes = FALSE)
axis(1)
axis(2, at = seq(-1, 1, 1))
box()
abline(h = 0, lty = 3)
```


# Answer Rate my Professor.

(a) 

$\overline{x}=-0.0883$

$\overline{y}=3.9983$

```{r}
x <- -0.0883

y <- 3.9983

intercept_2 <- 4.010

slope_2 <- (y-intercept_2)/x
slope_2


```

(b):

Slope is positive, so the linear relationship is positive. If we look at the scatter plot, we can see that the relationship is positive.

(c):


1- Linearity : The releationship between the explanatory and the response variable should be linear.  **This condition is met even though the relationship might be weak but there is a linear relationship.**

2- Nearly Normal Residuals: The residuals should be nearly normal. This condition may not be satisfied when there are unusual observations that dont follow the trend of the rest of the data. **This condition is met. If we look at the histogram of the residuals , we can see that the residuals are distributed normaly.**

3- Constant Variability: The variability of points around the least squares line should be roughly constant. This implies that the variability of residuals around the 0 line should be roughly constant as well. **This condition is met. If we look at the residuals scatter plot and qnorm and qline , we can see that there are few outliers on both ends. There arent any significant outliers. **



